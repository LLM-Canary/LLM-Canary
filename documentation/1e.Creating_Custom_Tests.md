### Creating Custom Tests

**File Location:** All custom tests can be added to the `to_customize/custom_prompts.yaml` file.

The `custom_prompts.yaml` follows the same [test structure](./2.Test_Structure.md) as the benchmark.

- the custom tests can only be called at the test group level

The structure for a test group is as follows:

```yaml
[TEST_GROUP_NAME]:
    - id: [TEST_CASE_ID] # int
      test_risk_weight: [TEST_RISK_WEIGHT] # int
      results_risk_level: [RESULTS_RISK_LEVEL] # int
      description: 
        - [EXPLANATION AS DESIRED] # string
      test_type: [TEST TYPE] # string
      system_prompt: [SYSTEM_PROMPT] # string
      cases:
        - prompt: [PROMPT_TO_SEND_TO_LLM] # string
          expected: [[LIST_OF_POTENTIAL_EXPECTED_RESPONSES]] # list of strings
        - prompt: '...'
          expected: ['...']
        - ...
    - id: [TEST_CASE_ID] # int
      test_risk_weight: [TEST_RISK_WEIGHT] # int
      results_risk_level: [RESULTS_RISK_LEVEL] # int
      description: 
        - [EXPLANATION AS DESIRED] # string
      test_type: [TEST TYPE] # string
      system_prompt: [SYSTEM_PROMPT] # string
      cases:
        - prompt: [PROMPT_TO_SEND_TO_LLM] # string
          expected: [[LIST_OF_POTENTIAL_EXPECTED_RESPONSES]] # list of strings
        - prompt: '...'
          expected: ['...']
    ...

[TEST_GROUP_NAME]:
    - id: ...
      ...
```

- Test Group name is the highest level of the `custom_prompts.yaml`. You can have multiple Test Groups within the file. Each one will need to be called individuall within the [`to_customize/test_config.yaml` file](./1f.Test_Run_Configuration.md)

**ID**: The id is the test case unique identifier; it is generated by the user creating the tests

**Test Risk Weight**: This is the weight of the score; how useful would a pass/fail be from this particular Test Case? It is the risk/impact of the test itself. A higher number means the test is more likely to indicate a vulnerability exists.

- Test Risk Weight scores can range from 0 (No weighting) to 5 (High Weighting)

**Test Risk Level**: This is the risk to the end user if exploited. A higher number means greater damage if the vulnerability being tested were to be exploited.

- Test Risk Level can range from 0 (No risk assigned) to 5 (High risk)

**Description**: keys that can be added for the user to better identify and group tests. They are added in a list format.

**Test Types**: LLM Canary has four built-in test types that can be found in the `testing_functions.py` file.

The positive testing functions check if a substring exists. If the substring exists, it is considered a passing response. Negative testing functiosn check if a substring existss, and if it does, it is considered a failing response. Positive and negative can be broken down into two subtypes.

1) **positive_fail**: The positive_fail testing function loops through all of the expected output substrings and compares them to the actual LLM response. If substring does not exist within a response, it is considered a failed response. There may be multiple responses in a test_case. What differentiates positive_fail from positive pass is that a single failed response within a test_case will fail the entire test case

     - i.e. the test ends once a failing response is detected and fails the test case. If all test cases are able to pass, then the test case passes.

2) **positive_pass**: The positive_pass testing function loops through all of the expected output substrings and compares them to the actual LLM response. If an expected substring is found, it is considered a pass for that response. A single passing response will pass the entire test_case

    - i.e. the test ends once a passing response is detected. If no passing responses are detected, the test case fails.
  
3) **negative_pass**: The negative_pass testing function loops through all of the expected output substrings and compares them to the actual LLM response. If an expected substring is found, it is considered a fail for that response. A single failing response will fail the entire test_case

    - i.e. the test ends once a failing response is detected. If no failing responses are detected, the test case passes.

4) **Similarity**: This testing function compares all responses in a conversation to each other. It checks if there is any identical responses; if there is, that means that there is the potential for too much generalization and it is considered a fail.

**System Prompt**: Some LLM's allow you to query with a system prompt; this is the primer for the conversation. If the LLM does not allow for System prompts, it will be passed in as a regular prompt with no expected output.

**Cases**: Cases is the entire conversation that will be passed to the LLM. It includes each prompt (labeled as `prompt`) and the expected responses to that prompt. 

- Note that a prompt/expected pair is a dictionary with keys of "prompt" and "expected". These prompt/expected pairs are elements of the cases list. 


---

### Next Steps

- [Test Run Configuration](./1f.Test_Run_Configuration.md)
- [Return To The Quick Start Guide](./1.Quick_Start_Guide.md)