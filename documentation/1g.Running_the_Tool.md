## Running the Tool

1. **Running the testing tool:** From the main menu of LLM Canary, select option 4 to start the testing tool.

![Main Menu](./../documentation/screenshots/main_menu_v0.png)

This will start the testing tool. The testing tool will run through each test case within the Test Suites and/or Test Groups declared in the [`to_customize/test_config.yaml`](./1f.Test_Run_Configuration.md)

At the start of each test case, the command line will display what LLM is being testing (LLM ID), What Test Group the Test Case is apart of (Test Group Name), and the Test Case identifier (Test Case ID).

![Testing_tool](./../documentation/screenshots/run_test_ID.png)

If there are any API errors while testing, they will be displayed under the test case printout where the error occured. The Test Case will then be skipped.

After all tests have completed running, a summary will be output into the Command Prompt. Users can choose to review the results there or review the results for all current and past test runs in the [scorecard](./1h.Accessing_the_Scorecard.md).

### Reading the Command Prompt Test Output

The first output will be a summary of all tests run.

![Pytest Results Summary](./../documentation/screenshots/pytest_results_summary.png)

Next will be a breakdown of the failed tests. It will show the function which ran the test, the type of error received, and then display the prompts, responses, and expected responses.

- Note: An assertion error means that it was a scripted test failure; it is not an error in the code

![Pytest Failure](./../documentation/screenshots/pytest_failure_breakdown.png)

### Reading the Scorecard Test Output

Please see the [accessing the scorecard](./1h.Accessing_the_Scorecard.md) page for more information about viewing the scorecard.

---

### Next Steps

- [Accessing The Scorecard](./1h.Accessing_the_Scorecard.md)
- [Return To The Quick Start Guide](./1.Quick_Start_Guide.md)