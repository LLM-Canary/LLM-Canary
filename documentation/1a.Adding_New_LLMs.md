# Adding new LLMs and Platforms
To add a new or custom model to the LLM-Canary tool you must clone the repository and add the custom code locally.

Model call functionality exists in the `to_customize/model_calls` folder. The LLM-Canary tool works on a per-platform basis, meaning that LLMs that run on the same code/have the same input/output formatting can be run using the same `model_call` code.

1. You must pick a unique identifier for your model. If you are using a publicly hosted platform, the unique identifier would be the name of the platform. If you are hosting locally, you can name it anything that does not conflict with any of the existing [platforms](./3.Platforms_and_Models.md).
2. in `to_customize/model_calls`, create a file called `run_[UID]` where `[UID]` is the unique identifier chosen in step 1.
3. the `run_[UID]` file must have the following three functions to run.
- **`clean_input(system_prompt, prompts)`**
     - This function is meant to take all prompts from a `test_case` and format them to be passed into the tool.
     - parameters:
          - `system_prompt`: str, a system prompt as defined in a test_case in either the `benchmark.yaml` or `custom_prompts.yaml`.
          - `prompts`: list, each element in the list is a prompt/expected response pair. each dict should only have two keys: `prompt` and `expected`. 
     - output:
          - `inputs`: list, a list of all prompts, including the system prompt, that will be sent to the LLM in str format.
          - `formatted_inputs`: list, Similar to inputs; however the conversation is what will be passed directly into the LLM. Each prompt must be formatted to what the LLM is expecting.
          - `expected`: list. a list of all expected responses, including the system prompt expected response. Each element in the list should be a list of expected responses.
     - **Example:** OpenAI requires inputs to be formatted in dictionaries with each prompt being a dictionary with two keys: `role` and `content`
        ```python
       def clean_input(system_prompt, prompts):
           formatted_inputs = []
           inputs = [system_prompt] # system prompt can be added directly to inputs as inputs is not formatted
           expected = [[]] # start with empty for system prompt
           formatted_inputs.append({'role': 'system', 'content': system_prompt}) # add formatting
           for prompt in prompts: # for the LLM to take a conversation.
               inputs.append(prompt['prompt'])
               formatted_inputs.append({'role': 'user', 'content': prompt['prompt']})
               expected.append(prompt['expected'])
           return inputs, formatted_inputs, expected

       ## prompts = ['hello world.', 'hello assistant.']
       ## clean_input(prompts) --> [{'user': 'hello world.'}, {'user': 'hello assistant.'}]
       ```
- **`run_model(env_var, messages, temp_bench)`**
     - This is the function that sends the API call to the LLM.
     - parameters
          - `env_var`: dict, a dictionary containing the parameters stored in the [.env file](./1c.LLM_Model_Environment_Configuration.md).
          - `messages`: list, This is the conversation gnerated in `clean_input()`
          - `temp_bench`: int, either 1 or -1. Used to check if the benchmark is being run. If `temp_bench` is -1, temperature is pulled from the env_vars. IF `temp_bench` is 1, the benchmark is being run and the temperature is set to 1.
     - outputs
          - `conversation`: list, contains both the prompts sent to the LLM and the responses received from the LLM in the format required by the model.
     - **Example:** [OpenAI API Request](https://platform.openai.com/docs/quickstart?context=python) guidance requires the use of the OpenAI module.
        ```python
       import openai

       def run_model(env_var, messages, temp_bench):
           # check if this is a benchmark test_case
           if temp_bench == 1:
               temp = 1
           else:
               temp = float(env_var['TEMPERATURE']) # .env stores variables as strings
           
           # API Call
           conversation = [] # create variable to store prompts and responses
           for message in messages:
               conversation.append(message) # add prompts to conversation
               response = openai.ChatCompletion.create(
                   model = env_var['MODEL'],
                   api_key = env_var['API-KEY'],
                   messages = conversation,
                   temperature = temp,)
               response_message = response['choices'][0]['message'] # pull response only
               conversation.append(response_message) # store response in conversation
           return conversation
        ```
- **`clean_output(conversation)`**
     - Takes the conversation from `run_model`, pulls out the responses, and reformats the contents for the tool.
     - parameters
       - `conversation`: list, contains both the prompts sent to the LLM and the responses received from the LLM in the format required by the model.
     - outputs
       - `responses_only`: list, a list of the string responses from the LLM. Removes any formatting required by the LLM:
     - **Example:** OpenAI returns responses as a dictionary where all responses from the LLM have a `role` of `assistant`
        ```python
        def clean_output(conversation):
            responses_only = []
            for comment in conversation:
                if comment['role'] == 'assistant':
                    responses_only.append(comment['content'])
            return responses_only

            ## response = [{'role': 'assistant', 'content': 'hello world.'}, {'role': 'assistant', 'content': 'hello user.'}]
            ## clean_output(response) --> ['hello world.', 'hello user.']
       ```

---

### Next Steps

- [Guided Model Configuration](./1b.LLM_Model_Guided_Configuration.md) OR [Environment Model Configuration](./1c.LLM_Model_Environment_Configuration.md)
- [Return To The Quick Start Guide](./1.Quick_Start_Guide.md)